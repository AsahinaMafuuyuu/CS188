# 不确定搜索
## 简述： 
#### 给定当前状态，未来和过去则是独立的
#### 行动结果仅取决于当前状态 
放置代理的环境可能使代理的行为具有不确定性，这意味着在某些状态下采取的操作可能导致多种可能的后续状态。
![alt text](image-7.png)
## 1. 马尔可夫决策过程（Markov Decision Processes）
马尔可夫决策过程定义如下：  
- 一组状态S（MDP的状态表示方式和传统搜索问题的状态表示方式一样）
- 一组操作A（也与传统表示方式一致）
- 起始状态
- 可能的**一个或者多个**终止状态
- 可能的一个**折现因子(discount factor)γ**  
- 一个**过渡函数***T(s,a,s′)*，实质上是一个概率函数，**表示智能体从状态s采取行动a最终进入状态s'的一个概率**  
- **奖励函数***R(s,a,s′)*，通常，MDP在每一步都有小的“生存”奖励，以奖励代理的生存，以及到达最终状态的大奖励。奖励可能是积极的，也可能是消极的，这取决于它们是否有利于所讨论的代理，代理的目标自然是在到达某个终端状态之前获得可能的最大奖励

此外可以用U来表达奖励函数的总和：
$$U([s_0,a_0,s_1,a_1,s_2,...])=R(s_0,a_0,s_1)+R(s_1,a_1,s_2)+R(s_2,a_2,s_3)+...$$

### 1.1 有限视界与折现（Finite Horizons and Discounting）
将折扣因子引入到奖励函数中，具体说就是，折现因子为γ，在时间步长为t时刻将状态s<sub>t</sub>采取行动a<sub>t</sub>后并进入到状态s<sub>t+1</sub>,则奖励函数用**γ<sup>t</sup>R(s<sub>t</sub>,a<sub>t</sub>,s<sub>t+1</sub>)** 代替掉原来的奖励函数 **R(s<sub>t</sub>,a<sub>t</sub>,s<sub>t+1</sub>)**（即初始状态t=0时奖励函数仍然是R(s<sub>t</sub>,a<sub>t</sub>,s<sub>t+1</sub>)）

因此，加入折扣因子之后，奖励函数发生变化，其总和也发生变化：
$$\begin{aligned}U([s_0,s_1,s_2,...])&=\quad R(s_0,a_0,s_1)+\gamma R(s_1,a_1,s_2)+\gamma^2R(s_2,a_2,s_3)+...\\&=\quad\sum_{t=0}^\infty\gamma^tR(s_t,a_t,s_{t+1})\leq\sum_{t=0}^\infty\gamma^tR_{max}=\boxed{\frac{R_{max}}{1-\gamma}}\end{aligned}$$
其中R<sub>max</sub>就是MDP中任意给定时间步时可获得的最大可能奖励。


